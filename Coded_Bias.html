<!-- This is an HTML page for the JNL221 class' first repository.-->

<!DOCTYPE html>
<html>
	<head>
	
		<meta charset="utf-8">
		<title>First repository</title>
		<link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,400,300,600,700&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
		<link href='https://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>
	
		<!-- This is where this page's style sheet is defined. -->
		<link type="text/css" rel="stylesheet" href="index.css" />

	</head>
	<body>
	
		<!-- This is the header row for this page. -->
		<div id="intro">
			<h1>DANNY ROVI</h1>
			<h4>Syracuse University, Fall 2025</h4>
		</div>

		<section>
			<p> 1. One part of the movie that really surprised me was when Joy showed that some facial recognition systems couldn’t even detect her face until she put on a plain white mask. I always thought that this kind of technology was extremely advanced, efficient, and accurate, so it shocked me to see how badly it failed on women and people with darker skin. That made me realize that algorithms are in fact not neutral. They reflect the people who make them, which includes their biases. I learned that technology isn’t always fair, and it can make inequality worse if no one is paying attention.</p>
			<p>2. In modern day pop culture, I usually see AI shown as something really advanced or even scary, like robots in movies that take over the world or personal assistants that seem smarter than humans. Shows and movies often make AI seem almost perfect, like it can do anything without making mistakes. However, Coded Bias presented AI in a different sense. The documentary showed that algorithms are far from perfect; they can be biased and unfair, and sometimes even detrimental to society. The biggest difference is that mainstream media often makes AI look powerful and futuristic, while the documentary focused on the flaws and real human impact of these systems right now in the current moment.</p>
			<p>Question One: What is the probability that chatbots produce sexist, racist, or homophobic language compared to finding alternate ways to respond to hateful prompts? Question Two: What specific defenses are installed to try and prevent chatbots from producing this language? Are there any? If so, what are they? Hypothesis: If it is possible for an AI chatbot to produce biased “hate speech”, should there be greater legal action and regulations to ensure that bots remain unbiased and only produce factual information?
</p>



</p>
		</section>

		<div id="end">
			<h4>this page was published on github pages. fonts: montserrat, open sans.</h4>
		</div>


	</body>
</html>
